{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "234dcb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-27 17:54:22.210759: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from seligator.common.params import MetadataEncoding\n",
    "from seligator.simple_demo import prepare_model, train_and_get\n",
    "from seligator.tests.evaluate import *\n",
    "from seligator.models.siamese import SiameseClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f5e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset reader set with following categories: [msd], lemma_char, lemma\n",
      "/home/thibault/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:55: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  warnings.warn(\n",
      "INFO:root:Indexer set for following categories: [msd], lemma_char, lemma\n",
      "INFO:root:TSV READER uses following metadata encoding MetadataEncoding.IGNORE \n",
      "INFO:root:Reading data\n",
      "WARNING:allennlp.data.fields.multilabel_field:Your label namespace was '[msd]'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "INFO:root:Building the vocabulary\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41184e784de2414a831b9f075b80f552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/5427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "INFO:allennlp.training.optimizers:Number of trainable parameters: 3252682\n",
      "INFO:root:Num epochs: 20\n",
      "INFO:root:Starting training\n",
      "INFO:allennlp.training.gradient_descent_trainer:Beginning training.\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch 0/19\n",
      "INFO:allennlp.training.gradient_descent_trainer:Worker 0 memory usage: 2.7G\n",
      "INFO:allennlp.training.gradient_descent_trainer:GPU 0 memory usage: 15M\n",
      "INFO:allennlp.training.gradient_descent_trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epochs:   20\n",
      "---> Patience: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca51f5a44703491cb400aaa8ad187e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/est-lascivuum-non-est/seligator/models/classifier.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits)\n",
      "INFO:allennlp.training.callbacks.console_logger:Batch inputs\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/[msd] (Shape: 4 x 55 x 52)\n",
      "tensor([[[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma_char/lemma_char/token_characters (Shape: 4 x 55 x 12)\n",
      "tensor([[[ 7, 15, 17,  ...,  0,  0,  0],\n",
      "         [24,  3, 12,  ...,  0,  0,  0],\n",
      "         [ 4, 20,  6,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 9,  6,  5,  ...,  0,  0,  0],\n",
      "         [19,  2,  3,  ...,  0,  0,  0],\n",
      "         [ 2,  9,  2,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 5, 12,  3,  ...,  0,  0,  0],\n",
      "         [ 9,  6,  5,  ...,  0,  0,  0],\n",
      "         [14,  6, 14,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[18,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 2,  8,  4,  ...,  0,  0,  0],\n",
      "         [15,  6,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 5,  2, 11,  ...,  0,  0,  0],\n",
      "         [14,  4,  8,  ...,  0,  0,  0],\n",
      "         [23,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma/lemma/tokens (Shape: 4 x 55)\n",
      "tensor([[  16,   11,   19,  ...,    0,    0,    0],\n",
      "        [  57,   99,   72,  ...,    0,    0,    0],\n",
      "        [ 413,   78,  173,  ...,    0,    0,    0],\n",
      "        [   2,  207,   53,  ...,    5, 2345,    3]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/label (Shape: 4)\n",
      "tensor([0, 0, 0, 1], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
      "INFO:allennlp.training.gradient_descent_trainer:Validating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8995479618294f5fa0f9834fccc2056c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.callbacks.console_logger:Batch inputs\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/[msd] (Shape: 4 x 35 x 52)\n",
      "tensor([[[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma_char/lemma_char/token_characters (Shape: 4 x 35 x 12)\n",
      "tensor([[[19,  2,  3,  ...,  0,  0,  0],\n",
      "         [ 4, 10, 20,  ...,  0,  0,  0],\n",
      "         [18,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 4,  8, 17,  ...,  0,  0,  0],\n",
      "         [ 7,  2, 15,  ...,  0,  0,  0],\n",
      "         [34, 13,  4,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19,  2,  7,  ...,  0,  0,  0],\n",
      "         [ 4,  9,  3,  ...,  0,  0,  0],\n",
      "         [ 7, 15,  4,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [11,  2, 13,  ...,  0,  0,  0],\n",
      "         [ 5,  2, 11,  ...,  0,  0,  0],\n",
      "         [23,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19,  2,  3,  ...,  0,  0,  0],\n",
      "         [ 5,  2,  5,  ...,  0,  0,  0],\n",
      "         [ 7, 21,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma/lemma/tokens (Shape: 4 x 35)\n",
      "tensor([[  34,  112,    2,  ...,    0,    0,    0],\n",
      "        [   4,  169,  996,  ...,    0,    0,    0],\n",
      "        [1387,   40,  665,  ..., 5058,    5,    3],\n",
      "        [  34, 1572,   18,  ...,    0,    0,    0]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/label (Shape: 4)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
      "INFO:allennlp.training.callbacks.console_logger:                       Training |  Validation\n",
      "INFO:allennlp.training.callbacks.console_logger:accuracy           |     0.796  |     0.888\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore             |     0.760  |     0.875\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore-negative    |     0.852  |     0.915\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore-positive    |     0.668  |     0.835\n",
      "INFO:allennlp.training.callbacks.console_logger:gpu_0_memory_MB    |    14.708  |       N/A\n",
      "INFO:allennlp.training.callbacks.console_logger:loss               |     0.431  |     0.289\n",
      "INFO:allennlp.training.callbacks.console_logger:precision          |     0.813  |     0.897\n",
      "INFO:allennlp.training.callbacks.console_logger:precision-negative |     0.780  |     0.876\n",
      "INFO:allennlp.training.callbacks.console_logger:precision-positive |     0.846  |     0.919\n",
      "INFO:allennlp.training.callbacks.console_logger:recall             |     0.745  |     0.862\n",
      "INFO:allennlp.training.callbacks.console_logger:recall-negative    |     0.938  |     0.958\n",
      "INFO:allennlp.training.callbacks.console_logger:recall-positive    |     0.552  |     0.766\n",
      "INFO:allennlp.training.callbacks.console_logger:worker_0_memory_MB |  2781.543  |       N/A\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch duration: 0:00:19.093673\n",
      "INFO:allennlp.training.gradient_descent_trainer:Estimated training time remaining: 0:06:01\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch 1/19\n",
      "INFO:allennlp.training.gradient_descent_trainer:Worker 0 memory usage: 2.7G\n",
      "INFO:allennlp.training.gradient_descent_trainer:GPU 0 memory usage: 90M\n",
      "INFO:allennlp.training.gradient_descent_trainer:Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db708ec2f868455abe2e3d2ec5ca16c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run tests on Vectors Categories\n",
    "\n",
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    use_han=True,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\"batch_size\": 4, \"metadata_encoding\": MetadataEncoding.IGNORE},\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "        #    \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": True},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    ),\n",
    "    #batches_per_epoch=100,\n",
    "    # model_class=SiameseClassifier,\n",
    "    use_bert_higway=True,\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd.csv\"\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    use_han=True,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\"batch_size\": 4, \"metadata_encoding\": MetadataEncoding.AS_TOKEN},\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "        #    \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": True},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    ),\n",
    "    #batches_per_epoch=100,\n",
    "    # model_class=SiameseClassifier,\n",
    "    use_bert_higway=True,\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd-metadatatoken.csv\"\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9194a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    use_han=True,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\n",
    "        \"batch_size\": 4,\n",
    "        \"metadata_encoding\": MetadataEncoding.AS_TOKEN,\n",
    "        \"metadata_tokens_categories\": (\"WrittenType\", )\n",
    "    },\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "        #    \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": True},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    )\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd-metadatatoken.csv\"\n",
    ")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf1b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    use_han=True,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\n",
    "        \"batch_size\": 4,\n",
    "        \"metadata_encoding\": MetadataEncoding.AS_TOKEN,\n",
    "        \"metadata_tokens_categories\": (\"Textgroup\", )\n",
    "    },\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "        #    \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": True},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    )\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=10,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd-metadatatoken.csv\"\n",
    ")\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
