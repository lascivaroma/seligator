{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f4dbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 10:47:47.505082: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:root:Dataset reader set with following categories: [msd], lemma_char, lemma\n",
      "/home/thibault/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:55: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  warnings.warn(\n",
      "INFO:root:Indexer set for following categories: [msd], lemma_char, lemma\n",
      "WARNING:root:TSV READER uses following metadata encoding MetadataEncoding.IGNORE \n",
      "WARNING:root:TSV Reader keeps following metadata Century, Textgroup, WrittenType, CitationTypes\n",
      "INFO:root:Reading data\n",
      "WARNING:allennlp.data.fields.multilabel_field:Your label namespace was '[msd]'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "INFO:root:Building the vocabulary\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/21955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Adding OOV to following fields: \n",
      "INFO:root:Fitting the BasisVectorConfiguration\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading pretrained embeddings from file\n",
      "WARNING:allennlp.modules.token_embedders.embedding:The embeddings file has an unknown file extension \".header\". We will assume the file is an (uncompressed) text file\n",
      "INFO:allennlp.modules.token_embedders.embedding:Recognized a header line in the embedding file with number of tokens: 155131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10273/4233259927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mseligator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mseligator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeligator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./seligator/best-nometadata.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AdamW\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/main.py\u001b[0m in \u001b[0;36minit_from_params\u001b[0;34m(token_features, msd_features, bert_dir, seq2vec_encoder_type, model_class, additional_model_kwargs, batches_per_epoch, reader_kwargs, encoder_hidden_size, agglomerate_msd, use_bert_highway, model_embedding_kwargs, basis_vector_configuration, training, vocabulary_path, folder)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0minput_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             mixed_encoder=MixedEmbeddingEncoder.build(\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0memb_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMixedEmbeddingEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_default_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/modules/mixed_encoders/__init__.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(cls, vocabulary, emb_dims, input_features, features_encoder, char_encoders, use_bert, bert_embedder, bert_pooler, mixer, emb_dropout, model_embedding_kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mmodel_embedding_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     ) -> \"MixedEmbeddingEncoder\":\n\u001b[0;32m--> 243\u001b[0;31m         emb = cls.build_embeddings(\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0minput_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/modules/mixed_encoders/__init__.py\u001b[0m in \u001b[0;36mbuild_embeddings\u001b[0;34m(vocabulary, input_features, pretrained_embeddings, trainable_embeddings, emb_dims, char_encoders, keep_all_vocab)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# input_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         emb = {\n\u001b[0m\u001b[1;32m    190\u001b[0m             cat: Embedding(\n\u001b[1;32m    191\u001b[0m                 \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/modules/mixed_encoders/__init__.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# input_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         emb = {\n\u001b[0;32m--> 190\u001b[0;31m             cat: Embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m                 \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_dim, num_embeddings, projection_dim, weight, padding_index, trainable, max_norm, norm_type, scale_grad_by_freq, sparse, vocab_namespace, pretrained_file, vocab)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# extend_vocab method, which relies on the value of vocab_namespace being None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;31m# to infer at what stage the embedding has been constructed. Phew.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             weight = _read_pretrained_embeddings_file(\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0mpretrained_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             )\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36m_read_pretrained_embeddings_file\u001b[0;34m(file_uri, embedding_dim, vocab, namespace)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read_embeddings_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read_embeddings_from_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/modules/token_embedders/embedding.py\u001b[0m in \u001b[0;36m_read_embeddings_from_text_file\u001b[0;34m(file_uri, embedding_dim, vocab, namespace)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_to_keep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                 \u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                     \u001b[0;31m# Sometimes there are funny unicode parsing problems that lead to different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from seligator.main import train_and_get, Seligator\n",
    "from seligator.common.load_save import load\n",
    "from seligator.prediction.tests import run_tests\n",
    "\n",
    "seligator, reader, train, dev = Seligator.init_from_params(**load(\"./seligator/best-nometadata.json\"))\n",
    "train_kwargs = dict(patience=4, num_epochs=20, lr=5e-4, optimizer=\"AdamW\")\n",
    "\n",
    "model = train_and_get(seligator.model, train, dev, **train_kwargs)\n",
    "data, img = run_tests(\n",
    "    \"dataset/main/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=f\"test.best.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seligator.save_model(\"saved_model_nometadata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
