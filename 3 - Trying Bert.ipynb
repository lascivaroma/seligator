{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2876da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'est', '[UNK]']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.fields import TextField, LabelField, Field\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer, \\\n",
    "    PretrainedTransformerIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WhitespaceTokenizer\n",
    "\n",
    "from seligator.common.constants import CATS, BERT_DIR\n",
    "from typing import Dict, Iterable, Tuple, List, Optional\n",
    "\n",
    "from transformers import BertTokenizer, XLNetTokenizer, WordpieceTokenizer\n",
    "from tokenizers.models import WordPiece, WordLevel\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "\n",
    "model = WordPiece.from_file(f\"{BERT_DIR}/latin_bert/vocab.txt\")\n",
    "tokenizer = Tokenizer(model)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "#PretrainedTransformerIndexer(\n",
    "#    model_name=f\"{BERT_DIR}/latin_bert/vocab.txt\",\n",
    "#    namespace=\"token_bert\"\n",
    "#)\n",
    "e = tokenizer.encode(\"id est lascivum\")\n",
    "e.tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f25b9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seligator.common.constants import CATS, BERT_DIR\n",
    "\n",
    "from typing import List, Optional, Set\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from tensor2tensor.data_generators.text_encoder import SubwordTextEncoder\n",
    "from allennlp.data.tokenizers.token_class import Token\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "class SubwordTextEncoderTokenizer(Tokenizer):\n",
    "    # https://github.com/tensorflow/tensor2tensor/blob/78ba8019847426e988294fd58f8953d7990a8db7/tensor2tensor/data_generators/text_encoder.py#L448\n",
    "    def __init__(self, \n",
    "                 vocab: str, \n",
    "                 add_special_tokens: bool = True, \n",
    "                 max_length: Optional[int] = None,\n",
    "                 special_tokens: Set[str] = None\n",
    "                ):\n",
    "        self.vocab = vocab\n",
    "        self._tokenizer = SubwordTextEncoder(vocab)\n",
    "        self._max_length = max_length\n",
    "        self._add_special_tokens = add_special_tokens\n",
    "        \n",
    "        self._special_tokens = special_tokens or {\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"}\n",
    "        \n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        \"\"\"\n",
    "        This method only handles a single sentence (or sequence) of text.\n",
    "        \"\"\"\n",
    "        max_length = self._max_length\n",
    "        if max_length is not None and not self._add_special_tokens:\n",
    "            max_length += self.num_special_tokens_for_sequence()\n",
    "\n",
    "        token_ids = self._tokenizer.encode(text)\n",
    "        token_texts = self._tokenizer.decode_list(token_ids)\n",
    "        special_tokens_mask = [1 if tok in self._special_tokens else 0 for tok in token_texts]\n",
    "        \n",
    "        tokens = []\n",
    "        for token_id, token_text, special_token_mask in zip(\n",
    "            token_ids, token_texts, special_tokens_mask\n",
    "        ):\n",
    "            if not self._add_special_tokens and special_token_mask == 1:\n",
    "                continue\n",
    "\n",
    "            tokens.append(\n",
    "                Token(\n",
    "                    text=token_text,\n",
    "                    text_id=token_id,\n",
    "                    type_id=None,\n",
    "                    idx=None,\n",
    "                    idx_end=None,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return tokens\n",
    "\n",
    "tokenizer = SubwordTextEncoderTokenizer(f\"{BERT_DIR}/latin_bert/vocab.txt\")\n",
    "z = tokenizer.tokenize(\"id est lascivumque pluchrum.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f9bc899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import logging\n",
    "import torch\n",
    "from allennlp.common.util import pad_sequence_to_length\n",
    "\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.tokenizers import Token, PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers.token_indexer import TokenIndexer, IndexedTokenList\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SubwordTextEncoderIndexer(TokenIndexer):\n",
    "    \"\"\"\n",
    "    This `TokenIndexer` assumes that Tokens already have their indexes in them (see `text_id` field).\n",
    "    We still require `model_name` because we want to form allennlp vocabulary from pretrained one.\n",
    "    This `Indexer` is only really appropriate to use if you've also used a\n",
    "    corresponding :class:`PretrainedTransformerTokenizer` to tokenize your input.  Otherwise you'll\n",
    "    have a mismatch between your tokens and your vocabulary, and you'll get a lot of UNK tokens.\n",
    "    Registered as a `TokenIndexer` with name \"pretrained_transformer\".\n",
    "    # Parameters\n",
    "    model_name : `str`\n",
    "        The name of the `transformers` model to use.\n",
    "    namespace : `str`, optional (default=`tags`)\n",
    "        We will add the tokens in the pytorch_transformer vocabulary to this vocabulary namespace.\n",
    "        We use a somewhat confusing default value of `tags` so that we do not add padding or UNK\n",
    "        tokens to this namespace, which would break on loading because we wouldn't find our default\n",
    "        OOV token.\n",
    "    max_length : `int`, optional (default = `None`)\n",
    "        If not None, split the document into segments of this many tokens (including special tokens)\n",
    "        before feeding into the embedder. The embedder embeds these segments independently and\n",
    "        concatenate the results to get the original document representation. Should be set to\n",
    "        the same value as the `max_length` option on the `PretrainedTransformerEmbedder`.\n",
    "    tokenizer_kwargs : `Dict[str, Any]`, optional (default = `None`)\n",
    "        Dictionary with\n",
    "        [additional arguments](https://github.com/huggingface/transformers/blob/155c782a2ccd103cf63ad48a2becd7c76a7d2115/transformers/tokenization_utils.py#L691)\n",
    "        for `AutoTokenizer.from_pretrained`.\n",
    "    \"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        namespace: str = \"tags\",\n",
    "        max_length: int = None,\n",
    "        tokenizer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self._namespace = namespace\n",
    "        self._allennlp_tokenizer = PretrainedTransformerTokenizer(\n",
    "            model_name, tokenizer_kwargs=tokenizer_kwargs\n",
    "        )\n",
    "        self._tokenizer = self._allennlp_tokenizer.tokenizer\n",
    "        self._added_to_vocabulary = False\n",
    "\n",
    "        self._num_added_start_tokens = len(self._allennlp_tokenizer.single_sequence_start_tokens)\n",
    "        self._num_added_end_tokens = len(self._allennlp_tokenizer.single_sequence_end_tokens)\n",
    "\n",
    "        self._max_length = max_length\n",
    "        if self._max_length is not None:\n",
    "            num_added_tokens = len(self._allennlp_tokenizer.tokenize(\"a\")) - 1\n",
    "            self._effective_max_length = (  # we need to take into account special tokens\n",
    "                self._max_length - num_added_tokens\n",
    "            )\n",
    "            if self._effective_max_length <= 0:\n",
    "                raise ValueError(\n",
    "                    \"max_length needs to be greater than the number of special tokens inserted.\"\n",
    "                )\n",
    "\n",
    "    def _add_encoding_to_vocabulary_if_needed(self, vocab: Vocabulary) -> None:\n",
    "        \"\"\"\n",
    "        Copies tokens from ```transformers``` model's vocab to the specified namespace.\n",
    "        \"\"\"\n",
    "        if self._added_to_vocabulary:\n",
    "            return\n",
    "\n",
    "        vocab.add_transformer_vocab(self._tokenizer, self._namespace)\n",
    "\n",
    "        self._added_to_vocabulary = True\n",
    "\n",
    "    @overrides\n",
    "    def count_vocab_items(self, token: Token, counter: Dict[str, Dict[str, int]]):\n",
    "        # If we only use pretrained models, we don't need to do anything here.\n",
    "        pass\n",
    "\n",
    "    @overrides\n",
    "    def tokens_to_indices(self, tokens: List[Token], vocabulary: Vocabulary) -> IndexedTokenList:\n",
    "        self._add_encoding_to_vocabulary_if_needed(vocabulary)\n",
    "\n",
    "        indices, type_ids = self._extract_token_and_type_ids(tokens)\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "        output: IndexedTokenList = {\n",
    "            \"token_ids\": indices,\n",
    "            \"mask\": [True] * len(indices),\n",
    "            \"type_ids\": type_ids or [0] * len(indices),\n",
    "        }\n",
    "\n",
    "        return self._postprocess_output(output)\n",
    "\n",
    "    @overrides\n",
    "    def indices_to_tokens(\n",
    "        self, indexed_tokens: IndexedTokenList, vocabulary: Vocabulary\n",
    "    ) -> List[Token]:\n",
    "        self._add_encoding_to_vocabulary_if_needed(vocabulary)\n",
    "\n",
    "        token_ids = indexed_tokens[\"token_ids\"]\n",
    "        type_ids = indexed_tokens.get(\"type_ids\")\n",
    "\n",
    "        return [\n",
    "            Token(\n",
    "                text=vocabulary.get_token_from_index(token_ids[i], self._namespace),\n",
    "                text_id=token_ids[i],\n",
    "                type_id=type_ids[i] if type_ids is not None else None,\n",
    "            )\n",
    "            for i in range(len(token_ids))\n",
    "        ]\n",
    "\n",
    "    def _extract_token_and_type_ids(self, tokens: List[Token]) -> Tuple[List[int], List[int]]:\n",
    "        \"\"\"\n",
    "        Roughly equivalent to `zip(*[(token.text_id, token.type_id) for token in tokens])`,\n",
    "        with some checks.\n",
    "        \"\"\"\n",
    "        indices: List[int] = []\n",
    "        type_ids: List[int] = []\n",
    "        for token in tokens:\n",
    "            indices.append(\n",
    "                token.text_id\n",
    "                if token.text_id is not None\n",
    "                else self._tokenizer.convert_tokens_to_ids(token.text)\n",
    "            )\n",
    "            type_ids.append(token.type_id if token.type_id is not None else 0)\n",
    "        return indices, type_ids\n",
    "\n",
    "    def _postprocess_output(self, output: IndexedTokenList) -> IndexedTokenList:\n",
    "        \"\"\"\n",
    "        Takes an IndexedTokenList about to be returned by `tokens_to_indices()` and adds any\n",
    "        necessary postprocessing, e.g. long sequence splitting.\n",
    "        The input should have a `\"token_ids\"` key corresponding to the token indices. They should\n",
    "        have special tokens already inserted.\n",
    "        \"\"\"\n",
    "        if self._max_length is not None:\n",
    "            # We prepare long indices by converting them to (assuming max_length == 5)\n",
    "            # [CLS] A B C [SEP] [CLS] D E F [SEP] ...\n",
    "            # Embedder is responsible for folding this 1-d sequence to 2-d and feed to the\n",
    "            # transformer model.\n",
    "            # TODO(zhaofengw): we aren't respecting word boundaries when segmenting wordpieces.\n",
    "\n",
    "            indices = output[\"token_ids\"]\n",
    "            type_ids = output.get(\"type_ids\", [0] * len(indices))\n",
    "\n",
    "            # Strips original special tokens\n",
    "            indices = indices[\n",
    "                self._num_added_start_tokens : len(indices) - self._num_added_end_tokens\n",
    "            ]\n",
    "            type_ids = type_ids[\n",
    "                self._num_added_start_tokens : len(type_ids) - self._num_added_end_tokens\n",
    "            ]\n",
    "\n",
    "            # Folds indices\n",
    "            folded_indices = [\n",
    "                indices[i : i + self._effective_max_length]\n",
    "                for i in range(0, len(indices), self._effective_max_length)\n",
    "            ]\n",
    "            folded_type_ids = [\n",
    "                type_ids[i : i + self._effective_max_length]\n",
    "                for i in range(0, len(type_ids), self._effective_max_length)\n",
    "            ]\n",
    "\n",
    "            # Adds special tokens to each segment\n",
    "            folded_indices = [\n",
    "                self._tokenizer.build_inputs_with_special_tokens(segment)\n",
    "                for segment in folded_indices\n",
    "            ]\n",
    "            single_sequence_start_type_ids = [\n",
    "                t.type_id for t in self._allennlp_tokenizer.single_sequence_start_tokens\n",
    "            ]\n",
    "            single_sequence_end_type_ids = [\n",
    "                t.type_id for t in self._allennlp_tokenizer.single_sequence_end_tokens\n",
    "            ]\n",
    "            folded_type_ids = [\n",
    "                single_sequence_start_type_ids + segment + single_sequence_end_type_ids\n",
    "                for segment in folded_type_ids\n",
    "            ]\n",
    "            assert all(\n",
    "                len(segment_indices) == len(segment_type_ids)\n",
    "                for segment_indices, segment_type_ids in zip(folded_indices, folded_type_ids)\n",
    "            )\n",
    "\n",
    "            # Flattens\n",
    "            indices = [i for segment in folded_indices for i in segment]\n",
    "            type_ids = [i for segment in folded_type_ids for i in segment]\n",
    "\n",
    "            output[\"token_ids\"] = indices\n",
    "            output[\"type_ids\"] = type_ids\n",
    "            output[\"segment_concat_mask\"] = [True] * len(indices)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @overrides\n",
    "    def get_empty_token_list(self) -> IndexedTokenList:\n",
    "        output: IndexedTokenList = {\"token_ids\": [], \"mask\": [], \"type_ids\": []}\n",
    "        if self._max_length is not None:\n",
    "            output[\"segment_concat_mask\"] = []\n",
    "        return output\n",
    "\n",
    "    @overrides\n",
    "    def as_padded_tensor_dict(\n",
    "        self, tokens: IndexedTokenList, padding_lengths: Dict[str, int]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        tensor_dict = {}\n",
    "        for key, val in tokens.items():\n",
    "            if key == \"type_ids\":\n",
    "                padding_value = 0\n",
    "                mktensor = torch.LongTensor\n",
    "            elif key == \"mask\" or key == \"wordpiece_mask\":\n",
    "                padding_value = False\n",
    "                mktensor = torch.BoolTensor\n",
    "            elif len(val) > 0 and isinstance(val[0], bool):\n",
    "                padding_value = False\n",
    "                mktensor = torch.BoolTensor\n",
    "            else:\n",
    "                padding_value = self._tokenizer.pad_token_id\n",
    "                if padding_value is None:\n",
    "                    padding_value = (\n",
    "                        0  # Some tokenizers don't have padding tokens and rely on the mask only.\n",
    "                    )\n",
    "                mktensor = torch.LongTensor\n",
    "\n",
    "            tensor = mktensor(\n",
    "                pad_sequence_to_length(\n",
    "                    val, padding_lengths[key], default_value=lambda: padding_value\n",
    "                )\n",
    "            )\n",
    "\n",
    "            tensor_dict[key] = tensor\n",
    "        return tensor_dict\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, PretrainedTransformerIndexer):\n",
    "            for key in self.__dict__:\n",
    "                if key == \"_tokenizer\":\n",
    "                    # This is a reference to a function in the huggingface code, which we can't\n",
    "                    # really modify to make this clean.  So we special-case it.\n",
    "                    continue\n",
    "                if self.__dict__[key] != other.__dict__[key]:\n",
    "                    return False\n",
    "            return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
