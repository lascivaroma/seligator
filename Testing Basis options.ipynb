{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 16:43:12.962105: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:root:Dataset reader set with following categories: [msd], lemma_char, lemma\n",
      "/home/thibault/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:55: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  warnings.warn(\n",
      "INFO:root:Indexer set for following categories: [msd], lemma_char, lemma\n",
      "INFO:root:TSV READER uses following metadata encoding MetadataEncoding.AS_CATEGORICAL \n",
      "INFO:root:Reading data\n",
      "WARNING:allennlp.data.fields.multilabel_field:Your label namespace was '[msd]'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "INFO:root:Building the vocabulary\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c38bc0f892546dda17102f51b560293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/5427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting the BasisVectorConfiguration\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading pretrained embeddings from file\n",
      "WARNING:allennlp.modules.token_embedders.embedding:The embeddings file has an unknown file extension \".header\". We will assume the file is an (uncompressed) text file\n",
      "INFO:allennlp.modules.token_embedders.embedding:Recognized a header line in the embedding file with number of tokens: 155131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6202c05c858d4a6ab37c562affa66d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.modules.token_embedders.embedding:Initializing pre-trained embedding layer\n",
      "INFO:allennlp.modules.token_embedders.embedding:Pretrained embeddings were found for 155131 out of 155381 tokens\n",
      "INFO:root:Classifier is using MetadataEnrichedLinear\n",
      "INFO:allennlp.training.optimizers:Number of trainable parameters: 1224434\n",
      "INFO:root:Current Optimizer: AdamWOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0.01\n",
      ") \n",
      "INFO:root:Num epochs: 20\n",
      "INFO:root:Starting training\n",
      "INFO:allennlp.training.gradient_descent_trainer:Beginning training.\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch 0/19\n",
      "INFO:allennlp.training.gradient_descent_trainer:Worker 0 memory usage: 2.9G\n",
      "INFO:allennlp.training.gradient_descent_trainer:GPU 0 memory usage: 125M\n",
      "INFO:allennlp.training.gradient_descent_trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epochs:   20\n",
      "---> Patience: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a4ff8be974a30a09d0a0a5a26c469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/est-lascivuum-non-est/seligator/models/classifier.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits)\n",
      "INFO:allennlp.training.callbacks.console_logger:Batch inputs\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/[msd] (Shape: 4 x 31 x 52)\n",
      "tensor([[[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma_char/lemma_char/token_characters (Shape: 4 x 31 x 11)\n",
      "tensor([[[ 4,  8, 17,  ...,  0,  0,  0],\n",
      "         [ 3, 15,  4,  ...,  0,  0,  0],\n",
      "         [22,  3,  6,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 5,  6, 13,  ...,  0,  0,  0],\n",
      "         [ 4,  2,  4,  ...,  0,  0,  0],\n",
      "         [23,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[24,  3, 12,  ...,  0,  0,  0],\n",
      "         [ 6, 14,  8,  ...,  0,  0,  0],\n",
      "         [28,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19,  2,  3,  ...,  0,  0,  0],\n",
      "         [14, 10,  6,  ...,  0,  0,  0],\n",
      "         [ 8,  7,  9,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[14,  3, 14,  ...,  0,  0,  0],\n",
      "         [18,  0,  0,  ...,  0,  0,  0],\n",
      "         [10,  2,  8,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma/lemma/tokens (Shape: 4 x 31)\n",
      "tensor([[   4,  221,   88,  ...,  244,  817,    3],\n",
      "        [  11, 1203,   12,  ...,    0,    0,    0],\n",
      "        [  34,  341,  205,  ...,    0,    0,    0],\n",
      "        [1111,    2, 2275,  ...,    0,    0,    0]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/label (Shape: 4)\n",
      "tensor([1, 0, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Century (Shape: 4)\n",
      "tensor([0, 3, 5, 1], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Textgroup (Shape: 4)\n",
      "tensor([ 69, 167,   3,  60], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_WrittenType (Shape: 4)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_CitationTypes (Shape: 4)\n",
      "tensor([24,  1,  4,  7], device='cuda:0')\n",
      "INFO:allennlp.training.gradient_descent_trainer:Validating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de886b9ee56346f4a5333aed7ca7e5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.callbacks.console_logger:Batch inputs\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/[msd] (Shape: 4 x 35 x 52)\n",
      "tensor([[[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma_char/lemma_char/token_characters (Shape: 4 x 35 x 12)\n",
      "tensor([[[19,  2,  3,  ...,  0,  0,  0],\n",
      "         [ 4, 10, 20,  ...,  0,  0,  0],\n",
      "         [18,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 4,  8, 17,  ...,  0,  0,  0],\n",
      "         [ 7,  2, 15,  ...,  0,  0,  0],\n",
      "         [34, 13,  4,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19,  2,  7,  ...,  0,  0,  0],\n",
      "         [ 4,  9,  3,  ...,  0,  0,  0],\n",
      "         [ 7, 15,  4,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [11,  2, 13,  ...,  0,  0,  0],\n",
      "         [ 5,  2, 11,  ...,  0,  0,  0],\n",
      "         [23,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[19,  2,  3,  ...,  0,  0,  0],\n",
      "         [ 5,  2,  5,  ...,  0,  0,  0],\n",
      "         [ 7, 21,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma/lemma/tokens (Shape: 4 x 35)\n",
      "tensor([[  34,  112,    2,  ...,    0,    0,    0],\n",
      "        [   4,  169,  996,  ...,    0,    0,    0],\n",
      "        [1387,   40,  665,  ..., 5058,    5,    3],\n",
      "        [  34, 1572,   18,  ...,    0,    0,    0]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/label (Shape: 4)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Century (Shape: 4)\n",
      "tensor([2, 0, 2, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Textgroup (Shape: 4)\n",
      "tensor([ 2,  4, 12,  4], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_WrittenType (Shape: 4)\n",
      "tensor([0, 0, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_CitationTypes (Shape: 4)\n",
      "tensor([0, 6, 8, 6], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:                       Training |  Validation\n",
      "INFO:allennlp.training.callbacks.console_logger:accuracy           |     0.861  |     0.872\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore             |     0.849  |     0.868\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore-negative    |     0.890  |     0.891\n",
      "INFO:allennlp.training.callbacks.console_logger:fscore-positive    |     0.808  |     0.845\n",
      "INFO:allennlp.training.callbacks.console_logger:gpu_0_memory_MB    |   125.024  |       N/A\n",
      "INFO:allennlp.training.callbacks.console_logger:loss               |     0.319  |     0.337\n",
      "INFO:allennlp.training.callbacks.console_logger:precision          |     0.852  |     0.863\n",
      "INFO:allennlp.training.callbacks.console_logger:precision-negative |     0.880  |     0.959\n",
      "INFO:allennlp.training.callbacks.console_logger:precision-positive |     0.825  |     0.767\n",
      "INFO:allennlp.training.callbacks.console_logger:recall             |     0.846  |     0.886\n",
      "INFO:allennlp.training.callbacks.console_logger:recall-negative    |     0.900  |     0.831\n",
      "INFO:allennlp.training.callbacks.console_logger:recall-positive    |     0.792  |     0.940\n",
      "INFO:allennlp.training.callbacks.console_logger:worker_0_memory_MB |  2936.980  |       N/A\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch duration: 0:00:25.652858\n",
      "INFO:allennlp.training.gradient_descent_trainer:Estimated training time remaining: 0:08:01\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch 1/19\n",
      "INFO:allennlp.training.gradient_descent_trainer:Worker 0 memory usage: 3.0G\n",
      "INFO:allennlp.training.gradient_descent_trainer:GPU 0 memory usage: 176M\n",
      "INFO:allennlp.training.gradient_descent_trainer:Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c215a1cf6dd4933bebc2a6dd90afd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seligator.common.params import MetadataEncoding, Seq2VecEncoderType, BasisVectorConfiguration\n",
    "from seligator.simple_demo import prepare_model, train_and_get\n",
    "from seligator.tests.evaluate import *\n",
    "from seligator.models.siamese import SiameseClassifier\n",
    "\n",
    "# Run tests on Vectors Categories\n",
    "METADATA_CATS = (\"Century\", \"Textgroup\", \"WrittenType\", \"CitationTypes\")\n",
    "BVC = BasisVectorConfiguration(\n",
    "    categories=METADATA_CATS\n",
    ")\n",
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    seq2vec_encoder_type=Seq2VecEncoderType.MetadataAttentionPooling,\n",
    "    basis_vector_configuration=BVC,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\n",
    "        \"batch_size\": 4, \n",
    "        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "        \"metadata_tokens_categories\": METADATA_CATS\n",
    "    },\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "            \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": False},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    ),\n",
    "    #batches_per_epoch=100,\n",
    "    # model_class=SiameseClassifier,\n",
    "    use_bert_higway=True,\n",
    "    additional_model_kwargs={\n",
    "        \"metadata_linear\": True,\n",
    "    }\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=2,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    #optimizer_params=dict(rho=0.9, eps=1e-6)\n",
    "#    use_cpu=True\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd-metadatacat-basis.csv\"\n",
    ")\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
