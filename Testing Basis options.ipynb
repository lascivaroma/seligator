{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-28 17:53:55.689740: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "INFO:root:Dataset reader set with following categories: [msd], lemma_char, lemma\n",
      "/home/thibault/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/data/token_indexers/token_characters_indexer.py:55: UserWarning: You are using the default value (0) of `min_padding_length`, which can cause some subtle bugs (more info see https://github.com/allenai/allennlp/issues/1954). Strongly recommend to set a value, usually the maximum size of the convolutional layer size when using CnnEncoder.\n",
      "  warnings.warn(\n",
      "INFO:root:Indexer set for following categories: [msd], lemma_char, lemma\n",
      "INFO:root:TSV READER uses following metadata encoding MetadataEncoding.AS_CATEGORICAL \n",
      "INFO:root:Reading data\n",
      "WARNING:allennlp.data.fields.multilabel_field:Your label namespace was '[msd]'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "INFO:root:Building the vocabulary\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a45532bb52046eab964ca9851928b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/5427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting the BasisVectorConfiguration\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading pretrained embeddings from file\n",
      "WARNING:allennlp.modules.token_embedders.embedding:The embeddings file has an unknown file extension \".header\". We will assume the file is an (uncompressed) text file\n",
      "INFO:allennlp.modules.token_embedders.embedding:Recognized a header line in the embedding file with number of tokens: 155131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04845be9c5424e6c9fffa17afc024c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/155131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.modules.token_embedders.embedding:Initializing pre-trained embedding layer\n",
      "INFO:allennlp.modules.token_embedders.embedding:Pretrained embeddings were found for 155131 out of 155381 tokens\n",
      "INFO:allennlp.training.optimizers:Number of trainable parameters: 1787058\n",
      "INFO:root:Current Optimizer: AdamWOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0.01\n",
      ") \n",
      "INFO:root:Num epochs: 20\n",
      "INFO:root:Starting training\n",
      "INFO:allennlp.training.gradient_descent_trainer:Beginning training.\n",
      "INFO:allennlp.training.gradient_descent_trainer:Epoch 0/19\n",
      "INFO:allennlp.training.gradient_descent_trainer:Worker 0 memory usage: 2.9G\n",
      "INFO:allennlp.training.gradient_descent_trainer:GPU 0 memory usage: 125M\n",
      "INFO:allennlp.training.gradient_descent_trainer:Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epochs:   20\n",
      "---> Patience: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3376e3613d46acbbf13a970fe086f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/dev/est-lascivuum-non-est/seligator/models/classifier.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(logits)\n",
      "INFO:allennlp.training.callbacks.console_logger:Batch inputs\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/[msd] (Shape: 4 x 33 x 52)\n",
      "tensor([[[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 1,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma_char/lemma_char/token_characters (Shape: 4 x 33 x 10)\n",
      "tensor([[[ 5,  4, 15,  ...,  0,  0,  0],\n",
      "         [12,  2, 11,  ...,  0,  0,  0],\n",
      "         [15,  4,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 3, 14,  5,  ...,  0,  0,  0],\n",
      "         [ 4,  8,  3,  ...,  0,  0,  0],\n",
      "         [18,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 2,  4,  9,  ...,  0,  0,  0],\n",
      "         [ 7, 10,  2,  ...,  0,  0,  0],\n",
      "         [23,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[34,  4, 11,  ...,  5,  0,  0],\n",
      "         [48,  4, 20,  ...,  5,  0,  0],\n",
      "         [18,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  4, 10,  ..., 15,  3,  7],\n",
      "         [11,  4,  2,  ...,  0,  0,  0],\n",
      "         [ 5, 12,  3,  ...,  8,  0,  0],\n",
      "         ...,\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0]]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/lemma/lemma/tokens (Shape: 4 x 33)\n",
      "tensor([[  24,   21,   25,  ...,    0,    0,    0],\n",
      "        [  28,   41,    2,  ...,   80, 4027,    3],\n",
      "        [4033, 8036,    2,  ...,    0,    0,    0],\n",
      "        [1887,   45,  413,  ...,    0,    0,    0]], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/label (Shape: 4)\n",
      "tensor([0, 1, 0, 0], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:Field : \"batch_input/metadata\" : (Length 4 of type \"<class 'dict'>\")\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Century (Shape: 4)\n",
      "tensor([0, 2, 1, 3], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_Textgroup (Shape: 4)\n",
      "tensor([ 0, 43, 90,  6], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_WrittenType (Shape: 4)\n",
      "tensor([0, 1, 0, 1], device='cuda:0')\n",
      "INFO:allennlp.training.callbacks.console_logger:batch_input/metadata_categoricals_CitationTypes (Shape: 4)\n",
      "tensor([ 1, 10,  7,  2], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 15, 128])\n",
      "torch.Size([4, 17, 128])\n",
      "torch.Size([4, 21, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 30, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 34, 128])\n",
      "torch.Size([4, 70, 128])\n",
      "torch.Size([4, 61, 128])\n",
      "torch.Size([4, 69, 128])\n",
      "torch.Size([4, 69, 128])\n",
      "torch.Size([4, 28, 128])\n",
      "torch.Size([4, 23, 128])\n",
      "torch.Size([4, 18, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 61, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 25, 128])\n",
      "torch.Size([4, 17, 128])\n",
      "torch.Size([4, 25, 128])\n",
      "torch.Size([4, 28, 128])\n",
      "torch.Size([4, 24, 128])\n",
      "torch.Size([4, 96, 128])\n",
      "torch.Size([4, 41, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 46, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 26, 128])\n",
      "torch.Size([4, 13, 128])\n",
      "torch.Size([4, 49, 128])\n",
      "torch.Size([4, 47, 128])\n",
      "torch.Size([4, 40, 128])\n",
      "torch.Size([4, 18, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 23, 128])\n",
      "torch.Size([4, 23, 128])\n",
      "torch.Size([4, 42, 128])\n",
      "torch.Size([4, 44, 128])\n",
      "torch.Size([4, 16, 128])\n",
      "torch.Size([4, 31, 128])\n",
      "torch.Size([4, 37, 128])\n",
      "torch.Size([4, 53, 128])\n",
      "torch.Size([4, 30, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 21, 128])\n",
      "torch.Size([4, 40, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 41, 128])\n",
      "torch.Size([4, 51, 128])\n",
      "torch.Size([4, 48, 128])\n",
      "torch.Size([4, 49, 128])\n",
      "torch.Size([4, 73, 128])\n",
      "torch.Size([4, 22, 128])\n",
      "torch.Size([4, 39, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 48, 128])\n",
      "torch.Size([4, 37, 128])\n",
      "torch.Size([4, 38, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 92, 128])\n",
      "torch.Size([4, 52, 128])\n",
      "torch.Size([4, 39, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 21, 128])\n",
      "torch.Size([4, 50, 128])\n",
      "torch.Size([4, 36, 128])\n",
      "torch.Size([4, 23, 128])\n",
      "torch.Size([4, 44, 128])\n",
      "torch.Size([4, 30, 128])\n",
      "torch.Size([4, 21, 128])\n",
      "torch.Size([4, 21, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 38, 128])\n",
      "torch.Size([4, 25, 128])\n",
      "torch.Size([4, 25, 128])\n",
      "torch.Size([4, 31, 128])\n",
      "torch.Size([4, 27, 128])\n",
      "torch.Size([4, 41, 128])\n",
      "torch.Size([4, 83, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 23, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 58, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 49, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 20, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 75, 128])\n",
      "torch.Size([4, 31, 128])\n",
      "torch.Size([4, 25, 128])\n",
      "torch.Size([4, 45, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 31, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 69, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 22, 128])\n",
      "torch.Size([4, 71, 128])\n",
      "torch.Size([4, 31, 128])\n",
      "torch.Size([4, 28, 128])\n",
      "torch.Size([4, 38, 128])\n",
      "torch.Size([4, 42, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 18, 128])\n",
      "torch.Size([4, 38, 128])\n",
      "torch.Size([4, 36, 128])\n",
      "torch.Size([4, 35, 128])\n",
      "torch.Size([4, 26, 128])\n",
      "torch.Size([4, 37, 128])\n",
      "torch.Size([4, 17, 128])\n",
      "torch.Size([4, 26, 128])\n",
      "torch.Size([4, 73, 128])\n",
      "torch.Size([4, 24, 128])\n",
      "torch.Size([4, 29, 128])\n",
      "torch.Size([4, 30, 128])\n",
      "torch.Size([4, 39, 128])\n",
      "torch.Size([4, 49, 128])\n",
      "torch.Size([4, 62, 128])\n",
      "torch.Size([4, 73, 128])\n",
      "torch.Size([4, 33, 128])\n",
      "torch.Size([4, 22, 128])\n",
      "torch.Size([4, 32, 128])\n",
      "torch.Size([4, 48, 128])\n"
     ]
    }
   ],
   "source": [
    "from seligator.common.params import MetadataEncoding, Seq2VecEncoderType, BasisVectorConfiguration\n",
    "from seligator.simple_demo import prepare_model, train_and_get\n",
    "from seligator.tests.evaluate import *\n",
    "from seligator.models.siamese import SiameseClassifier\n",
    "\n",
    "# Run tests on Vectors Categories\n",
    "METADATA_CATS = (\"Century\", \"Textgroup\", \"WrittenType\", \"CitationTypes\")\n",
    "BVC = BasisVectorConfiguration(\n",
    "    categories=METADATA_CATS\n",
    ")\n",
    "model, reader, train, dev = prepare_model(\n",
    "    input_features=(\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "    seq2vec_encoder_type=Seq2VecEncoderType.MetadataLSTM,\n",
    "    basis_vector_configuration=BVC,\n",
    "    agglomerate_msd=True,\n",
    "    reader_kwargs={\n",
    "        \"batch_size\": 4, \n",
    "        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "        \"metadata_tokens_categories\": METADATA_CATS\n",
    "    },\n",
    "    model_embedding_kwargs=dict(\n",
    "        keep_all_vocab=True,\n",
    "        pretrained_embeddings={\n",
    "            # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "        #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "            \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "        },\n",
    "        trainable_embeddings={\"token\": False, \"lemma\": False},\n",
    "        pretrained_emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "    ),\n",
    "    #batches_per_epoch=100,\n",
    "    # model_class=SiameseClassifier,\n",
    "    use_bert_higway=True,\n",
    "    additional_model_kwargs={\n",
    "        \"metadata_linear\": False,\n",
    "    }\n",
    ")\n",
    "model = train_and_get(\n",
    "    model, train, dev,\n",
    "    patience=2,\n",
    "    num_epochs=20,\n",
    "    lr=5e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    #optimizer_params=dict(rho=0.9, eps=1e-6)\n",
    "#    use_cpu=True\n",
    ")\n",
    "print(model)\n",
    "data = run_tests(\n",
    "    \"dataset/split/test.txt\",\n",
    "    dataset_reader=reader, model=model, dump=\"test.lemma-msd-metadatacat-basis.csv\"\n",
    ")\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
