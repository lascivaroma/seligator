{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "123c67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seligator.common.params import MetadataEncoding, Seq2VecEncoderType, BasisVectorConfiguration\n",
    "from seligator.main import train_and_get, Seligator\n",
    "from seligator.common.load_save import load\n",
    "from seligator.prediction.tests import run_tests\n",
    "\n",
    "from seligator.models.siamese import SiameseClassifier\n",
    "from seligator.models.triplet import TripletClassifier\n",
    "from seligator.models.classifier import FeatureEmbeddingClassifier\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "def get_json_fn(fn):\n",
    "    return f\"dumped-results/{fn}.json\"\n",
    "\n",
    "def already_done(fn):\n",
    "    return os.path.exists(get_json_fn(fn))\n",
    "\n",
    "def save_json(fn, obj):\n",
    "    with open(get_json_fn(fn), \"w\") as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "\n",
    "# Run tests on Vectors Categories\n",
    "def get_kwargs():\n",
    "    METADATA_CATS = (\"Century\", \"Textgroup\", \"WrittenType\", \"CitationTypes\")\n",
    "    BVC = BasisVectorConfiguration(\n",
    "        categories=METADATA_CATS\n",
    "    )\n",
    "    return dict(\n",
    "        token_features=(\"lemma_char\", \"lemma\"),\n",
    "        msd_features=(\"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\"),\n",
    "        seq2vec_encoder_type=Seq2VecEncoderType.LSTM,\n",
    "        basis_vector_configuration=BVC,\n",
    "        agglomerate_msd=False,\n",
    "        reader_kwargs={\n",
    "            \"batch_size\": 4, \n",
    "            \"metadata_encoding\": MetadataEncoding.IGNORE,\n",
    "            \"metadata_tokens_categories\": METADATA_CATS\n",
    "        },\n",
    "        model_embedding_kwargs=dict(\n",
    "            keep_all_vocab=True,\n",
    "            pretrained_embeddings={\n",
    "                # \"token\": \"~/Downloads/latin.embeddings\",\n",
    "            #    \"token\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.token.word2vec.kv\",\n",
    "            #    \"lemma\": \"~/dev/these/notebooks/4 - Detection/data/embs_models/model.lemma.word2vec.kv.header\"\n",
    "            },\n",
    "            trainable_embeddings={\"token\": False, \"lemma\": False},\n",
    "            emb_dims={\"token\": 200, \"lemma\": 200}\n",
    "        ),\n",
    "        encoder_hidden_size=64,\n",
    "        batches_per_epoch=None,\n",
    "        model_class=FeatureEmbeddingClassifier,\n",
    "        use_bert_highway=False,\n",
    "        bert_dir = \"./bert/latin_bert\"\n",
    "    )\n",
    "\n",
    "    \n",
    "def get_train_and_get_kwargs():\n",
    "    return dict(patience=4, num_epochs=20, lr=5e-4, optimizer=\"AdamW\")\n",
    "\n",
    "def jqs(data):\n",
    "    return \"-\".join(sorted(list(data)))\n",
    "\n",
    "def get_filename(params, prefix = \"model\"):\n",
    "    remaped = []\n",
    "    for key in sorted(list(params.keys())):\n",
    "        if isinstance(params[key], str) and not params[key]:\n",
    "            continue\n",
    "        if isinstance(params[key], str) and \"-\" in params[key]:\n",
    "            remaped.append(f\"{key}-\"+\"\".join([\n",
    "                \"\".join([\n",
    "                    subv[:3].lower().capitalize()\n",
    "                    for subv in v.split(\"_\")\n",
    "                ])\n",
    "                for v in params[key].split(\"-\")\n",
    "            ]))\n",
    "        else:\n",
    "            remaped.append(f\"{key}-{params[key]}\")\n",
    "    print(remaped)\n",
    "    return prefix+\"--\"+\"__\".join(remaped)\n",
    "\n",
    "def merge(source, destination):\n",
    "    \"\"\" Source = New , Destination = Default\n",
    "    run me with nosetests --with-doctest file.py\n",
    "\n",
    "    >>> a = { 'first' : { 'all_rows' : { 'pass' : 'dog', 'number' : '1' } } }\n",
    "    >>> b = { 'first' : { 'all_rows' : { 'fail' : 'cat', 'number' : '5' } } }\n",
    "    >>> merge(b, a) == { 'first' : { 'all_rows' : { 'pass' : 'dog', 'fail' : 'cat', 'number' : '5' } } }\n",
    "    True\n",
    "    \"\"\"\n",
    "    for key, value in source.items():\n",
    "        if isinstance(value, dict):\n",
    "            # get node or create one\n",
    "            node = destination.setdefault(key, {})\n",
    "            merge(value, node)\n",
    "        else:\n",
    "            destination[key] = value\n",
    "\n",
    "    return destination\n",
    "\n",
    "def run_and_save(model_name, prepare_model_kwargs, train_kwargs, model_name_prefix: str = \"model\"):\n",
    "    fn = f\"{model_name_prefix}-{model_name}\"\n",
    "    \n",
    "    if already_done(fn):\n",
    "        print(f\"Already trained {fn}\")\n",
    "        return {}\n",
    "    \n",
    "    seligator, reader, train, dev = Seligator.init_from_params(\n",
    "        **prepare_model_kwargs\n",
    "    )\n",
    "    _ = train_and_get(seligator.model, train, dev, **train_kwargs)\n",
    "    seligator.save_model(f\"./models/{fn}\")\n",
    "    data, img = run_tests(\n",
    "        f\"{prepare_model_kwargs.get('folder', 'dataset/main')}/test.txt\",\n",
    "        dataset_reader=reader, model=seligator.model, dump=f\"./models/{fn}/test.csv\"\n",
    "    )\n",
    "    out = {\n",
    "        fn: {\n",
    "            **{x:v for x, v in data.items() if isinstance(v, float)},\n",
    "            **train_kwargs\n",
    "        }\n",
    "    }\n",
    "    save_json(fn, out)\n",
    "    return out\n",
    "\n",
    "def get_siamese():\n",
    "    siamese = get_kwargs()\n",
    "    siamese[\"model_class\"] = TripletClassifier\n",
    "    #siamese[\"batches_per_epoch\"] = 20\n",
    "    siamese_train_kwargs = get_train_and_get_kwargs()\n",
    "    #siamese_train_kwargs[\"num_epochs\"] = siamese_train_kwargs[\"num_epochs\"] * int(1351 // siamese[\"batches_per_epoch\"])\n",
    "    siamese_train_kwargs[\"patience\"] = 10\n",
    "    return siamese, siamese_train_kwargs\n",
    "\n",
    "def get_classic():\n",
    "    return get_kwargs(), get_train_and_get_kwargs()\n",
    "\n",
    "RUNS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c2e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what can be JSONIFIED\n",
    "\n",
    "import json\n",
    "\n",
    "# https://stackoverflow.com/questions/24481852/serialising-an-enum-member-to-json\n",
    "\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    _PUBLIC_ENUMS = {\n",
    "        \"MetadataEncoding\": MetadataEncoding, \n",
    "        \"Seq2VecEncoderType\": Seq2VecEncoderType, \n",
    "        #\"BasisVectorConfiguration\": BasisVectorConfiguration\n",
    "    }\n",
    "    _PUBLIC_CLASSES = {\n",
    "        \"SiameseClassifier\": SiameseClassifier,\n",
    "        \"FeatureEmbeddingClassifier\": FeatureEmbeddingClassifier\n",
    "    }\n",
    "\n",
    "    def default(self, obj):\n",
    "        if type(obj) in CustomEncoder._PUBLIC_ENUMS.values():\n",
    "            return {\"__enum__\": str(obj)}\n",
    "        elif isinstance(obj, type):\n",
    "            if obj in CustomEncoder._PUBLIC_CLASSES.values():\n",
    "                return {\"__type__\": str(obj.__name__)}\n",
    "            else:\n",
    "                print(obj)\n",
    "        elif isinstance(obj, BasisVectorConfiguration):\n",
    "            return {\"__basis_vector_configuration__\": obj.to_dict()}\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "    @staticmethod\n",
    "    def object_hook(d):\n",
    "        if \"__enum__\" in d:\n",
    "            name, member = d[\"__enum__\"].split(\".\")\n",
    "            return getattr(CustomEncoder._PUBLIC_ENUMS[name], member)\n",
    "        elif \"__type__\" in d:\n",
    "            return CustomEncoder._PUBLIC_CLASSES[d[\"__type__\"]]\n",
    "        elif \"__basis_vector_configuration__\" in d:\n",
    "            return BasisVectorConfiguration.from_dict(d[\"__basis_vector_configuration__\"])\n",
    "        else:\n",
    "            return d\n",
    "\n",
    "\n",
    "PRE_LEMMA = {\n",
    "    \"model_embedding_kwargs\":{\n",
    "        \"pretrained_embeddings\": {\n",
    "            \"lemma\": \"embs/model.lemma.word2vec.kv.header\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6788d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_runs(get_kw, has_linear=False, prefix=\"model-\", folder=None):\n",
    "    if has_linear:\n",
    "        name = lambda string: \"Linear\"+string\n",
    "    else:\n",
    "        name = lambda string: \"Siamese\"+string\n",
    "        \n",
    "    NoAuthor_METADATA_CATS = (\"Century\", \"WrittenType\", \"CitationTypes\")\n",
    "    NoAuthor_BVC = BasisVectorConfiguration(\n",
    "        categories=NoAuthor_METADATA_CATS\n",
    "    )\n",
    "    NoAuthorCitation_METADATA_CATS = (\"Century\", \"WrittenType\")\n",
    "    NoAuthorCitation_BVC = BasisVectorConfiguration(\n",
    "        categories=NoAuthorCitation_METADATA_CATS\n",
    "    )\n",
    "    \n",
    "    PRE_LEMMA = {\n",
    "        \"model_embedding_kwargs\":{\n",
    "            \"pretrained_embeddings\": {\n",
    "                \"lemma\": \"embs/model.lemma.word2vec.kv.header\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    PRE_LEMMA_FASTTEXT = {\n",
    "        \"model_embedding_kwargs\":{\n",
    "            \"pretrained_embeddings\": {\n",
    "                \"lemma\": \"embs/model.lemma.fasttext.kv\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    Changes = [\n",
    "        # Use raw Bert\n",
    "        (name(\"BertTokenOnly\"), {\"token_features\": (\"token_subword\", )}, {}),\n",
    "        # Use raw Bert No Highway\n",
    "        (name(\"BertTokenOnlyWithHighway\"), {\"token_features\": (\"token_subword\", ), \"use_bert_highway\": True}, {}),\n",
    "        # Use raw Bert + Lemma\n",
    "        (name(\"BertLemma-HAN\"), {\n",
    "            \"token_features\": (\"token_subword\", \"lemma\", \"lemma_char\"),\n",
    "            \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "            \"use_bert_highway\": False\n",
    "        }, {}),\n",
    "        (name(\"Vanilla\"), {}, {}),\n",
    "        # Raw Features + MSD + Vanilla LSTM\n",
    "        (name(\"VanillaAggloMSD\"), {\"agglomerate_msd\": True}, {}),\n",
    "        # Raw Features + MSD + HAN\n",
    "        (name(\"VanillaAggloMSD-HAN\"), {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN}, {}),\n",
    "        # Raw Features + MSD + Enriched LSTM\n",
    "        (name(\"VanillaAggloMSD-EnriLSTM\"),\n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL}}, {}),\n",
    "        # Raw Features + MSD + Enriched Attention\n",
    "        (name(\"VanillaAggloMSD-EnriAttention\"),\n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataAttentionPooling,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL}}, {}),\n",
    "        # Raw Features + MSD + Attention\n",
    "        (name(\"VanillaAggloMSD-AttentPool\"),\n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.AttentionPooling,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL}}, {}),\n",
    "        # Now we use Metadata Tokens !\n",
    "        # Raw Features + MSD + Attention\n",
    "        #({ # Does not work because AttentionPooling expects metadata_vector\n",
    "        #    \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_TOKEN},\n",
    "        #    \"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.AttentionPooling}, {}),\n",
    "        # Raw Features + MSD + Attention\n",
    "        (name(\"VanillaAggloMSD-Metatoks-HAN\"),\n",
    "         {\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_TOKEN},\n",
    "            \"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN}, {}),\n",
    "        #\n",
    "        #\n",
    "        # With Pretrained\n",
    "        #\n",
    "        #\n",
    "        # Raw Features + MSD + Vanilla LSTM\n",
    "        (name(\"VanillaAggloMSD-Pretrained\"), {\"agglomerate_msd\": True, **PRE_LEMMA}, {}),\n",
    "        # Raw Features + MSD + HAN\n",
    "        (name(\"VanillaAggloMSD-HAN-Pretrained\"), {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "          **PRE_LEMMA}, {}),\n",
    "        # Raw Features + MSD + Enriched LSTM\n",
    "        (name(\"VanillaAggloMSD-EnriLSTM-Pretrained\"), \n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "          **PRE_LEMMA}, {}),\n",
    "        # Raw Features + MSD + Enriched Attention\n",
    "        (name(\"VanillaAggloMSD-EnriAttention-Pretrained\"), \n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataAttentionPooling,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "          **PRE_LEMMA}, {}),\n",
    "        # Raw Features + MSD + Attention\n",
    "        (name(\"VanillaAggloMSD-AttentPool-Pretrained\"), \n",
    "         {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.AttentionPooling,\n",
    "            \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "          **PRE_LEMMA}, {}),\n",
    "        # Morph, Agglo, LSTM Enriched Pas d'auteur \n",
    "        (\n",
    "            name(\"VanillaAggloMSD-EnriLSTM-Pretrained-NoAuthor\"), \n",
    "            {\n",
    "                \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                \"agglomerate_msd\": True,\n",
    "                \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "                \"reader_kwargs\": {\n",
    "                    \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                    \"metadata_tokens_categories\": NoAuthor_METADATA_CATS\n",
    "                },\n",
    "                \"basis_vector_configuration\": NoAuthor_BVC,\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": False},\n",
    "            **PRE_LEMMA\n",
    "        }, {}),\n",
    "        # Pas de Morph, LSTM Enriched Pas d'auteur\n",
    "        (\n",
    "            name(\"Vanilla-NoMorph-EnriLSTM-Pretrained-NoAuthor\"), \n",
    "            {\n",
    "                \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                \"msd_features\": [],\n",
    "                \"agglomerate_msd\": False,\n",
    "                \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "                \"reader_kwargs\": {\n",
    "                    \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                    \"metadata_tokens_categories\": NoAuthor_METADATA_CATS\n",
    "                },\n",
    "                \"basis_vector_configuration\": NoAuthor_BVC,\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": False},\n",
    "            **PRE_LEMMA\n",
    "        }, {}),\n",
    "        # Agglo, LSTM Enriched Pas d'auteur ni de Citation\n",
    "        (\n",
    "            name(\"VanillaAggloMSD-EnriLSTM-Pretrained-NoAuthorCitation\"), \n",
    "            {\n",
    "                \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                \"agglomerate_msd\": True,\n",
    "                \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "                \"reader_kwargs\": {\n",
    "                    \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                    \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                },\n",
    "                \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": False},\n",
    "            **PRE_LEMMA\n",
    "        }, {}),\n",
    "        # Pas de Morph, LSTM Enriched Pas d'auteur ni de Citation\n",
    "        (\n",
    "            name(\"Vanilla-NoMorph-EnriLSTM-Pretrained-NoAuthorCitation\"), \n",
    "            {\n",
    "                \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                \"msd_features\": [],\n",
    "                \"agglomerate_msd\": False,\n",
    "                \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "                \"reader_kwargs\": {\n",
    "                    \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                    \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                },\n",
    "                \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": False},\n",
    "            **PRE_LEMMA\n",
    "        }, {}),\n",
    "        # Pas de Morph, Agglo, LSTM Enriched Pas d'auteur ni de Citation, FastText\n",
    "        (\n",
    "            name(\"Vanilla-NoMorph-EnriLSTM-Pretrained-NoAuthorCitation-FastText\"), \n",
    "            {\n",
    "                \"token_features\": (\"lemma\", ), #No morph\n",
    "                \"msd_features\": [],\n",
    "                \"agglomerate_msd\": False,\n",
    "                \"seq2vec_encoder_type\": Seq2VecEncoderType.MetadataLSTM,\n",
    "                \"reader_kwargs\": {\n",
    "                    \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                    \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                },\n",
    "                \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": False},\n",
    "            **PRE_LEMMA_FASTTEXT\n",
    "        }, {}),\n",
    "    ]\n",
    "    if has_linear:\n",
    "        Changes = Changes + [\n",
    "            (\n",
    "                name(\"Vanilla-LinearEnriched\"), \n",
    "                 {\n",
    "                \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": True}\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"Vanilla-LinearEnriched-Pretrained\"), \n",
    "                {\n",
    "                \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"VanillaAggloMSD-HAN-LinearEnriched-Pretrained\"), \n",
    "                {\"agglomerate_msd\": True, \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            # Apparently, its the best, so let's play with input features\n",
    "            # (\"lemma_char\", \"lemma\", \"case\", \"numb\", \"gend\", \"mood\", \"tense\", \"voice\", \"person\", \"deg\")\n",
    "            (\n",
    "                name(\"Vanilla-NoMorph-HAN-LinearEnriched-Pretrained\"), \n",
    "                {\"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                 \"msd_features\": [],\n",
    "              \"agglomerate_msd\": False, \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                \"reader_kwargs\": {\"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL},\n",
    "                \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}\n",
    "            ),\n",
    "            # No Author in metadata\n",
    "            (\n",
    "                name(\"Vanilla-NoMorph-HAN-LinearEnriched-Pretrained-NoAuthor\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"msd_features\": [],\n",
    "                    \"agglomerate_msd\": True,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthor_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthor_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"Vanilla-NoMorph-HAN-LinearEnriched-Pretrained-NoAuthor\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"msd_features\": [],\n",
    "                    \"agglomerate_msd\": False,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthor_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthor_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"VanillaAggloMSD-HAN-LinearEnriched-Pretrained-NoAuthor\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"agglomerate_msd\": True,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthor_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthor_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            # No Author No Citation in metadata\n",
    "            (\n",
    "                name(\"Vanilla-NoMorph-HAN-LinearEnriched-Pretrained-NoAuthorCitation\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"msd_features\": [],\n",
    "                    \"agglomerate_msd\": True,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"Vanilla-NoMorph-HAN-LinearEnriched-Pretrained-NoAuthorCitation\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"msd_features\": [],\n",
    "                    \"agglomerate_msd\": False,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {}),\n",
    "            (\n",
    "                name(\"VanillaAggloMSD-HAN-LinearEnriched-Pretrained-NoAuthorCitation\"), \n",
    "                {\n",
    "                    \"token_features\": (\"lemma_char\", \"lemma\"), #No morph\n",
    "                    \"agglomerate_msd\": True,\n",
    "                    \"seq2vec_encoder_type\": Seq2VecEncoderType.HAN,\n",
    "                    \"reader_kwargs\": {\n",
    "                        \"metadata_encoding\": MetadataEncoding.AS_CATEGORICAL,\n",
    "                        \"metadata_tokens_categories\": NoAuthorCitation_METADATA_CATS\n",
    "                    },\n",
    "                    \"basis_vector_configuration\": NoAuthorCitation_BVC,\n",
    "                    \"additional_model_kwargs\": { \"metadata_linear\": True},\n",
    "                **PRE_LEMMA\n",
    "            }, {})\n",
    "        ]\n",
    "    for idx, (model_name, model_kw, train_kw) in enumerate(Changes):\n",
    "        defaults_model, default_trains = get_kw()\n",
    "        model_kw = merge(model_kw, defaults_model)\n",
    "        train_kw = merge(train_kw, default_trains)\n",
    "        if folder:\n",
    "            model_kw[\"folder\"] = folder\n",
    "        run_and_save(model_name, model_kw, train_kw, model_name_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fe1ce",
   "metadata": {},
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42cf700f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/latin_bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "WARNING:root:TSV READER uses following metadata encoding MetadataEncoding.IGNORE \n",
      "WARNING:root:TSV Reader keeps following metadata Century, Textgroup, WrittenType, CitationTypes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9651ef8d150404693015629d4d3c82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "building vocab:   0%|          | 0/21955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Adding OOV to following fields: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Epochs:   20\n",
      "---> Patience: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c937c35d17004ca19c9e9e92e6260f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5489 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7107/2517540920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# do_runs(get_classic, has_linear=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdo_runs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_siamese\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_linear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model-siamese-\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7107/3392401573.py\u001b[0m in \u001b[0;36mdo_runs\u001b[0;34m(get_kw, has_linear, prefix, folder)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mmodel_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"folder\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mrun_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_kw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7107/57431378.py\u001b[0m in \u001b[0;36mrun_and_save\u001b[0;34m(model_name, prepare_model_kwargs, train_kwargs, model_name_prefix)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mprepare_model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     )\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseligator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrain_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mseligator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./models/{fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     data, img = run_tests(\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/main.py\u001b[0m in \u001b[0;36mtrain_and_get\u001b[0;34m(model, train, dev, lr, use_cpu, return_metrics, **train_kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     metrics = train_model(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/training/trainer.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, dev_loader, cuda_device, patience, num_epochs, lr, optimizer, optimizer_params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36m_try_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs_completed\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_after_epochs_completed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                     \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                     \u001b[0mbatch_group_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/training/gradient_descent_trainer.py\u001b[0m in \u001b[0;36mbatch_outputs\u001b[0;34m(self, batch, for_training)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m \u001b[0madding\u001b[0m \u001b[0many\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mregularization\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/models/triplet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **inputs)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Need to take care of label ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mexm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_additional_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositiv_additional_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mneg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegativ_additional_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegativ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/modules/mixed_encoders/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, metadata_vector)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0membedded\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0madditional_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bert_projection\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0m_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/seligator/modules/mixed_encoders/__init__.py\u001b[0m in \u001b[0;36m_forward_features\u001b[0;34m(self, token, metadata_vector)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         ) = self.sort_and_run_forward(self._module, inputs, mask, hidden_state)\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Deal with the fact the LSTM state is a tuple of (state, memory).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/allennlp/modules/encoder_base.py\u001b[0m in \u001b[0;36msort_and_run_forward\u001b[0;34m(self, module, inputs, mask, hidden_state)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Actually call the module on the sorted PackedSequence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_sequence_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestoration_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/est-lascivuum-non-est/env/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[1;32m    665\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[1;32m    666\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# do_runs(get_classic, has_linear=True)\n",
    "do_runs(get_siamese, has_linear=False, prefix=\"model-siamese-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc281171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "RUNS = []\n",
    "for file in glob.glob(\"dumped-results/*.json\"):\n",
    "    with open(file) as f:\n",
    "        RUNS.append(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069cfee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best, best_key = 0, None\n",
    "sorts = sorted([(list(run.keys())[0], list(run.values())[0][\"accuracy\"], list(run.values())[0][\"fscore-positive\"]) for run in RUNS], key=lambda x: x[2])\n",
    "for key in sorts:\n",
    "    print(key[0], key[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22ba45",
   "metadata": {},
   "source": [
    "## Metaphors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90d779",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_runs(get_classic, folder=\"dataset/metaphors\", prefix=\"metaphors-\", has_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952bb14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_runs(get_classic, folder=\"dataset/inversed-metaphors\", prefix=\"inversed-metaphors-\", has_linear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d395f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "do_runs(get_classic, folder=\"dataset/main-partial\", prefix=\"main-partial-\", has_linear=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
