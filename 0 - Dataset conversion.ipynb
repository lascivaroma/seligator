{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f996514d",
   "metadata": {},
   "source": [
    "Rôle du notebook: conversion des fichiers XMLs en dataset TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "855cce9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#sexual\\nqui\\tqui1\\tPROrel\\tNom\\tSing\\tMasc\\t-\\t-\\t-\\t-\\t-\\nciet\\tcieo\\tVER\\t-\\tSing\\t-\\tInd\\tPres\\tAct\\t3\\t-\\ninritans\\tirrito\\tVER\\tNom\\tSing\\tCom\\tPar\\tPres\\tAct\\t-\\t-\\nloca\\tlocus\\tNOMcom\\tAcc\\tPlur\\t-\\t-\\t-\\t-\\t-\\t-\\nturgida\\tturgidus\\tADJqua\\tAcc\\tPlur\\tNeut\\t-\\t-\\t-\\t-\\tPos\\nsemine\\tsemen1\\tNOMcom\\tAbl\\tSing\\t-\\t-\\t-\\t-\\t-\\t-\\nmulto\\tmultus\\tADJqua\\tAbl\\tSing\\tMascNeut\\t-\\t-\\t-\\t-\\tPos\\n,\\t,\\tPUNC\\t-\\t-\\t-\\t-\\t-\\t-\\t-\\t-'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lxml.etree as ET\n",
    "\n",
    "CATS = (\"Case\", \"Numb\", \"Gend\", \"Mood\", \"Tense\", \"Voice\", \"Person\", \"Deg\")\n",
    "\n",
    "def msd_to_tsv(attrib, cats=CATS):\n",
    "    local_values = dict([\n",
    "        tuple(elem.split(\"=\"))\n",
    "        for elem in attrib.split(\"|\")\n",
    "        if elem.split(\"=\")[0] in cats\n",
    "    ])\n",
    "    return \"\\t\".join([\n",
    "        local_values.get(cat, \"-\")\n",
    "        for cat in cats\n",
    "    ])\n",
    "\n",
    "def file_to_string(fp: str, label: str, cats=CATS) -> str:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    with open(fp) as f:\n",
    "        xml = ET.parse(f)\n",
    "    return f\"#{label}\\n\"+\"\\n\".join([\n",
    "        f\"{token.text}\\t\"\n",
    "        f\"{token.attrib['lemma']}\\t\"\n",
    "        f\"{token.attrib['pos']}\\t\"\n",
    "        f\"{msd_to_tsv(token.attrib['msd'], cats=cats)}\"\n",
    "        for token in xml.xpath(\"//w\")\n",
    "    ])\n",
    "\n",
    "\n",
    "file_to_string(\"/home/thibault/dev/these-corpus/data/555.xml\", \"sexual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64392860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from typing import Dict\n",
    "\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer, TokenCharactersIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a308884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextField of length 5 with text: \n",
      " \t\t[The, best, movie, ever, !]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "LabelField with label: pos in namespace: 'labels'.\n",
      "SequenceLabelField of length 5 with labels:\n",
      " \t\t['DET', 'ADJ', 'NOUN', 'ADV', 'PUNKT']\n",
      " \t\tin namespace: 'labels'.\n",
      "5\n",
      "<generator object <genexpr> at 0x7fb979fd6bf8>\n",
      "TextField of length 0 with text: \n",
      " \t\t[]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'}\n",
      "LabelField with label: -1 in namespace: 'labels'.\n",
      "SequenceLabelField of length 0 with labels:\n",
      " \t\t[]\n",
      " \t\tin namespace: 'labels'.\n",
      "defaultdict(<class 'collections.Counter'>, {'tokens': Counter({'The': 1, 'best': 1, 'movie': 1, 'ever': 1, '!': 1})})\n",
      "defaultdict(<class 'collections.Counter'>, {'tokens': Counter({'The': 1, 'best': 1, 'movie': 1, 'ever': 1, '!': 1}), 'labels': Counter({'pos': 1})})\n",
      "defaultdict(<class 'collections.Counter'>, {'tokens': Counter({'The': 1, 'best': 1, 'movie': 1, 'ever': 1, '!': 1}), 'labels': Counter({'pos': 1, 'DET': 1, 'ADJ': 1, 'NOUN': 1, 'ADV': 1, 'PUNKT': 1})})\n",
      "{'tokens': {'tokens': tensor([2, 3, 4, 5, 6])}}\n",
      "tensor(0)\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# To create fields, simply pass the data to constructor.\n",
    "# NOTE: Don't worry about the token_indexers too much for now. We have a whole\n",
    "# chapter on why TextFields are set up this way, and how they work.\n",
    "tokens = [Token(\"The\"), Token(\"best\"), Token(\"movie\"), Token(\"ever\"), Token(\"!\")]\n",
    "token_indexers: Dict[str, TokenIndexer] = {\"tokens\": SingleIdTokenIndexer()}\n",
    "text_field = TextField(tokens, token_indexers=token_indexers)\n",
    "\n",
    "label_field = LabelField(\"pos\")\n",
    "\n",
    "sequence_label_field = SequenceLabelField(\n",
    "    [\"DET\", \"ADJ\", \"NOUN\", \"ADV\", \"PUNKT\"], text_field\n",
    ")\n",
    "\n",
    "# You can use print() fields to see their content\n",
    "print(text_field)\n",
    "print(label_field)\n",
    "print(sequence_label_field)\n",
    "\n",
    "# Many of the fields implement native python methods in intuitive ways\n",
    "print(len(sequence_label_field))\n",
    "print(label for label in sequence_label_field)\n",
    "\n",
    "# Fields know how to create empty fields of the same type\n",
    "print(text_field.empty_field())\n",
    "print(label_field.empty_field())\n",
    "print(sequence_label_field.empty_field())\n",
    "\n",
    "\n",
    "# You can count vocabulary items in fields\n",
    "counter: Dict[str, Dict[str, int]] = defaultdict(Counter)\n",
    "text_field.count_vocab_items(counter)\n",
    "print(counter)\n",
    "\n",
    "label_field.count_vocab_items(counter)\n",
    "print(counter)\n",
    "\n",
    "sequence_label_field.count_vocab_items(counter)\n",
    "print(counter)\n",
    "\n",
    "# Create Vocabulary for indexing fields\n",
    "vocab = Vocabulary(counter)\n",
    "\n",
    "# Fields know how to turn themselves into tensors\n",
    "text_field.index(vocab)\n",
    "# NOTE: in practice, we will batch together instances and use the maximum padding\n",
    "# lengths, instead of getting them from a single instance.\n",
    "# You can print this if you want to see what the padding_lengths dictionary looks\n",
    "# like, but it can sometimes be a bit cryptic.\n",
    "padding_lengths = text_field.get_padding_lengths()\n",
    "print(text_field.as_tensor(padding_lengths))\n",
    "\n",
    "label_field.index(vocab)\n",
    "print(label_field.as_tensor(label_field.get_padding_lengths()))\n",
    "\n",
    "sequence_label_field.index(vocab)\n",
    "padding_lengths = sequence_label_field.get_padding_lengths()\n",
    "print(sequence_label_field.as_tensor(padding_lengths))\n",
    "\n",
    "# Fields know how to batch tensors\n",
    "tensor1 = label_field.as_tensor(label_field.get_padding_lengths())\n",
    "\n",
    "label_field2 = LabelField(\"pos\")\n",
    "label_field2.index(vocab)\n",
    "tensor2 = label_field2.as_tensor(label_field2.get_padding_lengths())\n",
    "\n",
    "batched_tensors = label_field.batch_tensors([tensor1, tensor2])\n",
    "print(batched_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc0fec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CATS\n",
    "cats = CATS\n",
    "token_indexers: Dict[str, TokenIndexer] = {\n",
    "    \"tokens\": SingleIdTokenIndexer(namespace=\"token_vocab\"),\n",
    "    \"lemmas\": SingleIdTokenIndexer(namespace=\"lemma_vocab\"),\n",
    "    \"token_characters\": TokenCharactersIndexer(namespace=\"character_vocab\"),\n",
    "    **{\n",
    "        task.lower(): SingleIdTokenIndexer(namespace=f\"{task.lower()}_vocab\")\n",
    "        for task in cats\n",
    "    }\n",
    "}\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"This\", \"is\", \"some\", \"text\", \".\"], namespace=\"token_vocab\"\n",
    ")\n",
    "vocab.add_tokens_to_namespace(\n",
    "    [\"T\", \"h\", \"i\", \"s\", \" \", \"o\", \"m\", \"e\", \"t\", \"x\", \".\"], namespace=\"character_vocab\"\n",
    ")\n",
    "\n",
    "Besoin de forker un Tokenizer sachant que `, feature_name=\"tag_\"` indique d'où vient la donnée\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
